
==================================================
FILE: ./Makefile
==================================================
# PANTHEON Build System

# --- Compiler Configuration ---
HIPCC ?= hipcc
CXXFLAGS := -O3 -std=c++14 -DNDEBUG -Ikernels/common
BUILD_DIR := build

# Platform Detection
ifeq ($(PLATFORM), CUDA)
    COMPILER := nvcc
    # NVCC needs explicit architecture and treating .cpp as .cu
    CXXFLAGS += -x cu --gpu-architecture=sm_70 -Wno-deprecated-gpu-targets
else
    COMPILER := hipcc
    # HIPCC auto-detects architecture
    CXXFLAGS += -std=c++17 --offload-arch=native
endif

# --- Auto-Discovery Logic ---

# 1. Find all .cpp files inside kernels/ subdirectories
#    Result: kernels/hbm_write/hbm_write.cpp kernels/cache_latency/cache_latency.cpp ...
ALL_SRCS := $(shell find kernels -name "*.cpp")

# 2. Exclude anything in the 'common' directory or root headers
SRCS := $(filter-out kernels/common/%, $(ALL_SRCS))

# 3. Determine Target Binaries
#    We strip the directory path and extension to get the binary name.
#    Example: kernels/hbm_write/hbm_write.cpp -> build/hbm_write
BINS := $(patsubst kernels/%/%.cpp, $(BUILD_DIR)/%, $(SRCS))
# Handle cases where the cpp file might not match the folder name exactly, 
# or is just sitting in a folder. This is a fallback to flattened names.
# For PANTHEON, usually specific naming is used, but this is robust:
BINS := $(foreach src,$(SRCS),$(BUILD_DIR)/$(basename $(notdir $(src))))

# --- Targets ---

.PHONY: all clean directories

all: directories $(BINS)

# Create build directory
directories:
	@mkdir -p $(BUILD_DIR)

# Generic Rule: Build any binary from its corresponding source found in kernels tree
# The 'vpath' directive tells Make to look for .cpp files inside kernels/ and its subdirs
vpath %.cpp $(sort $(dir $(SRCS)))

$(BUILD_DIR)/%: %.cpp
	@echo "[BUILD] Compiling $< -> $@"
	@$(COMPILER) $(CXXFLAGS) $< -o $@

clean:
	@echo "[CLEAN] Removing build artifacts..."
	@rm -rf $(BUILD_DIR)

# Debug helper to see what Make found
debug:
	@echo "Found Sources: $(SRCS)"
	@echo "Target Bins:   $(BINS)"


==================================================
FILE: ./LICENSE
==================================================
                                 Apache License
                           Version 2.0, January 2004
                        http://www.apache.org/licenses/

   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION

   1. Definitions.

      "License" shall mean the terms and conditions for use, reproduction,
      and distribution as defined by Sections 1 through 9 of this document.

      "Licensor" shall mean the copyright owner or entity authorized by
      the copyright owner that is granting the License.

      "Legal Entity" shall mean the union of the acting entity and all
      other entities that control, are controlled by, or are under common
      control with that entity. For the purposes of this definition,
      "control" means (i) the power, direct or indirect, to cause the
      direction or management of such entity, whether by contract or
      otherwise, or (ii) ownership of fifty percent (50%) or more of the
      outstanding shares, or (iii) beneficial ownership of such entity.

      "You" (or "Your") shall mean an individual or Legal Entity
      exercising permissions granted by this License.

      "Source" form shall mean the preferred form for making modifications,
      including but not limited to software source code, documentation
      source, and configuration files.

      "Object" form shall mean any form resulting from mechanical
      transformation or translation of a Source form, including but
      not limited to compiled object code, generated documentation,
      and conversions to other media types.

      "Work" shall mean the work of authorship, whether in Source or
      Object form, made available under the License, as indicated by a
      copyright notice that is included in or attached to the work
      (an example is provided in the Appendix below).

      "Derivative Works" shall mean any work, whether in Source or Object
      form, that is based on (or derived from) the Work and for which the
      editorial revisions, annotations, elaborations, or other modifications
      represent, as a whole, an original work of authorship. For the purposes
      of this License, Derivative Works shall not include works that remain
      separable from, or merely link (or bind by name) to the interfaces of,
      the Work and Derivative Works thereof.

      "Contribution" shall mean any work of authorship, including
      the original version of the Work and any modifications or additions
      to that Work or Derivative Works thereof, that is intentionally
      submitted to Licensor for inclusion in the Work by the copyright owner
      or by an individual or Legal Entity authorized to submit on behalf of
      the copyright owner. For the purposes of this definition, "submitted"
      means any form of electronic, verbal, or written communication sent
      to the Licensor or its representatives, including but not limited to
      communication on electronic mailing lists, source code control systems,
      and issue tracking systems that are managed by, or on behalf of, the
      Licensor for the purpose of discussing and improving the Work, but
      excluding communication that is conspicuously marked or otherwise
      designated in writing by the copyright owner as "Not a Contribution."

      "Contributor" shall mean Licensor and any individual or Legal Entity
      on behalf of whom a Contribution has been received by Licensor and
      subsequently incorporated within the Work.

   2. Grant of Copyright License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      copyright license to reproduce, prepare Derivative Works of,
      publicly display, publicly perform, sublicense, and distribute the
      Work and such Derivative Works in Source or Object form.

   3. Grant of Patent License. Subject to the terms and conditions of
      this License, each Contributor hereby grants to You a perpetual,
      worldwide, non-exclusive, no-charge, royalty-free, irrevocable
      (except as stated in this section) patent license to make, have made,
      use, offer to sell, sell, import, and otherwise transfer the Work,
      where such license applies only to those patent claims licensable
      by such Contributor that are necessarily infringed by their
      Contribution(s) alone or by combination of their Contribution(s)
      with the Work to which such Contribution(s) was submitted. If You
      institute patent litigation against any entity (including a
      cross-claim or counterclaim in a lawsuit) alleging that the Work
      or a Contribution incorporated within the Work constitutes direct
      or contributory patent infringement, then any patent licenses
      granted to You under this License for that Work shall terminate
      as of the date such litigation is filed.

   4. Redistribution. You may reproduce and distribute copies of the
      Work or Derivative Works thereof in any medium, with or without
      modifications, and in Source or Object form, provided that You
      meet the following conditions:

      (a) You must give any other recipients of the Work or
          Derivative Works a copy of this License; and

      (b) You must cause any modified files to carry prominent notices
          stating that You changed the files; and

      (c) You must retain, in the Source form of any Derivative Works
          that You distribute, all copyright, patent, trademark, and
          attribution notices from the Source form of the Work,
          excluding those notices that do not pertain to any part of
          the Derivative Works; and

      (d) If the Work includes a "NOTICE" text file as part of its
          distribution, then any Derivative Works that You distribute must
          include a readable copy of the attribution notices contained
          within such NOTICE file, excluding those notices that do not
          pertain to any part of the Derivative Works, in at least one
          of the following places: within a NOTICE text file distributed
          as part of the Derivative Works; within the Source form or
          documentation, if provided along with the Derivative Works; or,
          within a display generated by the Derivative Works, if and
          wherever such third-party notices normally appear. The contents
          of the NOTICE file are for informational purposes only and
          do not modify the License. You may add Your own attribution
          notices within Derivative Works that You distribute, alongside
          or as an addendum to the NOTICE text from the Work, provided
          that such additional attribution notices cannot be construed
          as modifying the License.

      You may add Your own copyright statement to Your modifications and
      may provide additional or different license terms and conditions
      for use, reproduction, or distribution of Your modifications, or
      for any such Derivative Works as a whole, provided Your use,
      reproduction, and distribution of the Work otherwise complies with
      the conditions stated in this License.

   5. Submission of Contributions. Unless You explicitly state otherwise,
      any Contribution intentionally submitted for inclusion in the Work
      by You to the Licensor shall be under the terms and conditions of
      this License, without any additional terms or conditions.
      Notwithstanding the above, nothing herein shall supersede or modify
      the terms of any separate license agreement you may have executed
      with Licensor regarding such Contributions.

   6. Trademarks. This License does not grant permission to use the trade
      names, trademarks, service marks, or product names of the Licensor,
      except as required for reasonable and customary use in describing the
      origin of the Work and reproducing the content of the NOTICE file.

   7. Disclaimer of Warranty. Unless required by applicable law or
      agreed to in writing, Licensor provides the Work (and each
      Contributor provides its Contributions) on an "AS IS" BASIS,
      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
      implied, including, without limitation, any warranties or conditions
      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
      PARTICULAR PURPOSE. You are solely responsible for determining the
      appropriateness of using or redistributing the Work and assume any
      risks associated with Your exercise of permissions under this License.

   8. Limitation of Liability. In no event and under no legal theory,
      whether in tort (including negligence), contract, or otherwise,
      unless required by applicable law (such as deliberate and grossly
      negligent acts) or agreed to in writing, shall any Contributor be
      liable to You for damages, including any direct, indirect, special,
      incidental, or consequential damages of any character arising as a
      result of this License or out of the use or inability to use the
      Work (including but not limited to damages for loss of goodwill,
      work stoppage, computer failure or malfunction, or any and all
      other commercial damages or losses), even if such Contributor
      has been advised of the possibility of such damages.

   9. Accepting Warranty or Additional Liability. While redistributing
      the Work or Derivative Works thereof, You may choose to offer,
      and charge a fee for, acceptance of support, warranty, indemnity,
      or other liability obligations and/or rights consistent with this
      License. However, in accepting such obligations, You may act only
      on Your own behalf and on Your sole responsibility, not on behalf
      of any other Contributor, and only if You agree to indemnify,
      defend, and hold each Contributor harmless for any liability
      incurred by, or claims asserted against, such Contributor by reason
      of your accepting any such warranty or additional liability.

   END OF TERMS AND CONDITIONS

   APPENDIX: How to apply the Apache License to your work.

      To apply the Apache License to your work, attach the following
      boilerplate notice, with the fields enclosed by brackets "[]"
      replaced with your own identifying information. (Don't include
      the brackets!)  The text should be enclosed in the appropriate
      comment syntax for the file format. We also recommend that a
      file or class name and description of purpose be included on the
      same "printed page" as the copyright notice for easier
      identification within third-party archives.

   Copyright [yyyy] [name of copyright owner]

   Licensed under the Apache License, Version 2.0 (the "License");
   you may not use this file except in compliance with the License.
   You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.


==================================================
FILE: ./monitor.py
==================================================
import subprocess
import time
import json
import shutil
import threading
import numpy as np

class HardwareMonitor:
    def __init__(self, platform):
        self.platform = platform
        self.has_rocm_smi = shutil.which("rocm-smi") is not None
        self.has_nvidia_smi = shutil.which("nvidia-smi") is not None
        self.running = False
        self.history = {} 
        self.thread = None

    def get_gpu_count(self):
        if self.platform == "HIP" and self.has_rocm_smi:
            try:
                # AMD: Count keys in JSON output
                out = subprocess.check_output("rocm-smi -i --json", shell=True).decode()
                return len(json.loads(out))
            except: return 1
        elif self.has_nvidia_smi:
            try:
                out = subprocess.check_output("nvidia-smi --query-gpu=count --format=csv,noheader", shell=True).decode()
                return int(out.strip())
            except: return 1
        return 1

    def start_collection(self, gpu_ids):
        self.running = True
        self.history = {gid: {'temp': [], 'pwr': [], 'clk': []} for gid in gpu_ids}
        self.thread = threading.Thread(target=self._loop, args=(gpu_ids,))
        self.thread.start()

    def stop_collection(self):
        self.running = False
        if self.thread: self.thread.join()
        return self._aggregate()

    def _loop(self, gpu_ids):
        while self.running:
            if self.platform == "HIP" and self.has_rocm_smi:
                self._poll_amd(gpu_ids)
            elif self.has_nvidia_smi:
                self._poll_nvidia(gpu_ids)
            time.sleep(1)

    def _poll_amd(self, gpu_ids):
        try:
            # -P (Power), -t (Temp), -c (Clock), -m (Mem Clock)
            out = subprocess.check_output("rocm-smi -P -t -c -m --json", shell=True).decode()
            data = json.loads(out)
            
            for gid in gpu_ids:
                # card0, card1... or gpu0... try to find the key
                card_key = f"card{gid}"
                if card_key not in data: 
                    # Try finding by index if cardN naming fails
                    keys = list(data.keys())
                    if len(keys) > gid: card_key = keys[gid]
                
                if card_key in data:
                    c = data[card_key]
                    
                    # --- ROBUST KEY SEARCH ---
                    # 1. Temperature (Prioritize HBM/Junction, Fallback to Edge)
                    t_val = 0
                    for k, v in c.items():
                        k_lower = k.lower()
                        if "temp" in k_lower and "edge" in k_lower: 
                            t_val = float(v) # Baseline
                        if "temp" in k_lower and ("junction" in k_lower or "hotspot" in k_lower or "hbm" in k_lower):
                            t_val = float(v) # Upgrade to hotspot if found
                            break
                    if t_val > 0: self.history[gid]['temp'].append(t_val)

                    # 2. Power
                    p_val = 0
                    for k, v in c.items():
                        if "power" in k.lower() and "average" in k.lower():
                            p_val = float(v)
                            break
                        if "power" in k.lower() and float(v) > p_val:
                            p_val = float(v)
                    if p_val > 0: self.history[gid]['pwr'].append(p_val)

                    # 3. Clock (SCLK or MCLK)
                    clk_val = 0
                    for k, v in c.items():
                        if "sclk" in k.lower() and "(" in str(v):
                            # Format: "(1200Mhz)"
                            clean = str(v).replace("(", "").replace(")", "").replace("Mhz", "")
                            clk_val = float(clean)
                            break
                    if clk_val > 0: self.history[gid]['clk'].append(clk_val)

        except Exception as e:
            pass

    def _poll_nvidia(self, gpu_ids):
        try:
            cmd = "nvidia-smi --query-gpu=index,temperature.gpu,power.draw,clocks.gr --format=csv,noheader,nounits"
            out = subprocess.check_output(cmd, shell=True).decode()
            for line in out.strip().split('\n'):
                parts = line.split(',')
                idx = int(parts[0])
                if idx in self.history:
                    self.history[idx]['temp'].append(float(parts[1]))
                    self.history[idx]['pwr'].append(float(parts[2]))
                    self.history[idx]['clk'].append(float(parts[3]))
        except: pass

    def _aggregate(self):
        stats = {}
        for gid, data in self.history.items():
            # If no data collected (e.g. sensor failure), avoid crash with default 0
            stats[gid] = {
                "avg_temp": round(np.mean(data['temp']), 1) if data['temp'] else 0,
                "max_temp": round(np.max(data['temp']), 1) if data['temp'] else 0,
                "avg_pwr":  round(np.mean(data['pwr']), 1) if data['pwr'] else 0,
                "max_pwr":  round(np.max(data['pwr']), 1) if data['pwr'] else 0,
                "avg_clk":  round(np.mean(data['clk']), 0) if data['clk'] else 0,
            }
        return stats


==================================================
FILE: ./.gitignore
==================================================
# Prerequisites
*.d

# Object files
*.o
*.ko
*.obj
*.elf

# Linker output
*.ilk
*.map
*.exp

# Precompiled Headers
*.gch
*.pch

# Libraries
*.lib
*.a
*.la
*.lo

# Shared objects (inc. Windows DLLs)
*.dll
*.so
*.so.*
*.dylib

# Executables
*.exe
*.out
*.app
*.i*86
*.x86_64
*.hex

# Debug files
*.dSYM/
*.su
*.idb
*.pdb

# Kernel Module Compile Results
*.mod*
*.cmd
.tmp_versions/
modules.order
Module.symvers
Mkfile.old
dkms.conf

# debug information files
*.dwo

results/*
database/*
build/*


__pycache___


==================================================
FILE: ./README.md
==================================================
# Pantheon: Universal GPU Stress & Diagnostics Suite

Pantheon is a cross-platform (CUDA/ROCm) stress testing tool designed to isolate and hammer specific GPU subsystems. Unlike generic benchmarks (Furmark, 3DMark), Pantheon allows you to test specific silicon limits.

## Requirements
- **Python:** `pip3 install psutil pandas openpyxl numpy`
- **Compiler:** `nvcc` (NVIDIA) or `hipcc` (AMD)

## Quick Start
```bash
# Run the full suite (30 seconds per test)
python3 pantheon.py --test all --duration 30

# Run a specific "Power Virus" test
python3 pantheon.py --test tensor_virus --duration 60

Test RegistryTest NameTarget SubsystemFailure Symptomshbm_readVRAM Controller (Read)Screen artifacts, system freeze.hbm_write_aggInfinity Fabric / L2 CacheDriver timeout, massive stutter.tensor_virusTensor/Matrix Cores (FP16)Thermal Throttling, VRM shutdown (Black screen).atomic_virusROPs & L2 AtomicsApplication crashes, erratic performance.incineratorVector ALUs + SRAMGeneral instability, core clock drops.Interpretation of ResultsThroughput: For Read tests, this should match your card's theoretical max bandwidth.Power: tensor_virus should generate the highest power draw (W).Temps: incinerator or tensor_virus should generate the highest Hotspot temps.
### **Bonus: The "Burn-In" Script**
If you want to use this to test the stability of an overclock or a used GPU, create this shell script to run a 1-hour cyclic burn-in.

**Create File: `burn_in.sh`**

```bash
#!/bin/bash
echo "Starting 1-Hour Stability Burn-In..."

# 1. Heat up the loop (Matrix Cores) - 10 Minutes
echo "[Phase 1] Thermal Shock (Tensor Virus)"
python3 pantheon.py --test tensor_virus --duration 600

# 2. Thrash the Memory Controller - 10 Minutes
echo "[Phase 2] Memory Controller Saturation"
python3 pantheon.py --test hbm_read_agg --duration 600

# 3. Thrash the Cache/Fabric - 10 Minutes
echo "[Phase 3] Fabric/L2 Stress"
python3 pantheon.py --test atomic_virus --duration 600

# 4. Mixed Loop (All tests cycling) - 30 Minutes
echo "[Phase 4] Mixed Cycle"
for i in {1..5}
do
   python3 pantheon.py --test all --duration 60
done

echo "Burn-In Complete. Check database/ folder for logs."
You can make it executable with chmod +x burn_in.sh


==================================================
FILE: ./burn_in.sh
==================================================
#!/bin/bash
# Save this as burn_in.sh and run: chmod +x burn_in.sh

echo "Starting Pantheon 1-Hour Burn-In..."

# Phase 1: Thermal Shock (10 Mins)
# Rapidly heats up the die to test cooling mount pressure
echo "[Phase 1] Thermal Shock (Tensor Virus)"
python3 pantheon.py --test tensor_virus --duration 600

# Phase 2: VRAM Saturation (10 Mins)
# Fills memory to 99% to test for bad memory chips
echo "[Phase 2] Memory Controller Saturation"
python3 pantheon.py --test hbm_read_agg --duration 600

# Phase 3: Mixed Load (40 Mins)
# Cycles through all tests to find transient instability
echo "[Phase 3] Mixed Cycle Loop"
for i in {1..8}
do
   echo "Loop $i / 8"
   python3 pantheon.py --test all --duration 300
done

echo "Burn-In Complete. Check database/ folder for logs."


==================================================
FILE: ./pantheon.py
==================================================
import argparse
import subprocess
import time
import os
import sys
import threading
import datetime
import json
import platform
import shutil
import pandas as pd
import numpy as np
from monitor import HardwareMonitor

# Try to import psutil, warn if missing
try:
    import psutil
except ImportError:
    print("[PANTHEON] Warning: 'psutil' module not found. System info will be limited.")
    print("           Install it via: pip3 install psutil")
    psutil = None

# --- Configuration ---
BUILD_DIR = "build"
KERNEL_DIR = "kernels"
RESULTS_BASE_DIR = "results"
DATABASE_DIR = "database"

TEST_REGISTRY = {
    # Existing
    "hbm_write":      {"bin": "hbm_write",       "args": [], "desc": "HBM Write (Standard)"},
    "hbm_write_agg":  {"bin": "hbm_write_agg",   "args": [], "desc": "HBM Write (Aggressive)"},
    "hbm_read":       {"bin": "hbm_read",        "args": [], "desc": "HBM Read (Standard)"},
    "hbm_read_agg":   {"bin": "hbm_read_agg",    "args": [], "desc": "HBM Read (Aggressive)"},
    "voltage":        {"bin": "compute_virus",   "args": [], "desc": "Voltage Virus (ALU Hammer)"},
    "incinerator":    {"bin": "compute_virus_agg","args": [],"desc": "Incinerator (LDS Stress)"},
    "cache_lat":      {"bin": "cache_latency",   "args": [], "desc": "Cache Latency"},

    # NEW TESTS
    "tensor_virus":   {"bin": "tensor_virus",    "args": [], "desc": "Tensor Virus (FP16 Matrix Power)"},
    "atomic_virus":   {"bin": "atomic_virus",    "args": [], "desc": "Atomic Virus (L2 Cache Thrash)"}
}

def log(msg):
    print(f"[PANTHEON] {msg}")

def ensure_dir(path):
    if not os.path.exists(path): os.makedirs(path)

# --- System & GPU Info Gathering ---

def get_size(bytes, suffix="B"):
    """Scale bytes to proper format"""
    factor = 1024
    for unit in ["", "K", "M", "G", "T", "P"]:
        if bytes < factor:
            return f"{bytes:.2f}{unit}{suffix}"
        bytes /= factor

def get_gpu_static_info():
    """Detects static GPU details (Name, VRAM, Driver) via CLI tools."""
    gpu_list = []
    
    # 1. Try NVIDIA
    if shutil.which("nvidia-smi"):
        try:
            cmd = ["nvidia-smi", "--query-gpu=index,name,memory.total,driver_version", "--format=csv,noheader,nounits"]
            out = subprocess.check_output(cmd, encoding="utf-8").strip()
            for line in out.split('\n'):
                idx, name, mem, driver = line.split(", ")
                gpu_list.append({
                    "id": int(idx),
                    "type": "NVIDIA",
                    "name": name,
                    "memory_total": f"{mem} MB",
                    "driver_version": driver
                })
        except: pass

    # 2. Try AMD
    if shutil.which("rocm-smi") and not gpu_list:
        try:
            # ROCm SMI logic is trickier to parse cleanly in one go, usually returns JSON
            out = subprocess.check_output("rocm-smi --showproductname --showmeminfo vram --json", shell=True, encoding="utf-8")
            data = json.loads(out)
            # data structure: {"card0": {"...": "..."}}
            for key, val in data.items():
                idx = int(key.replace("card", ""))
                # Extract Name
                name = val.get("Card Series", "Unknown AMD GPU")
                # Extract VRAM
                vram = val.get("VRAM Total Memory (B)", "0")
                if vram != "0":
                    vram = f"{int(vram) // (1024*1024)} MB"
                
                gpu_list.append({
                    "id": idx,
                    "type": "AMD",
                    "name": name,
                    "memory_total": vram,
                    "driver_version": "ROCm Driver" # Hard to get exact kernel module version via simple smi
                })
        except: pass

    return gpu_list

def get_toolkit_version(platform_name):
    """Get CUDA or ROCm/HIP version."""
    version = "Unknown"
    if platform_name == "CUDA":
        try:
            out = subprocess.check_output(["nvcc", "--version"], encoding="utf-8")
            for line in out.split('\n'):
                if "release" in line:
                    version = line.split("release ")[1].split(",")[0]
        except: pass
    elif platform_name == "HIP":
        try:
            # Try to read local version file common on ROCm installs
            if os.path.exists("/opt/rocm/.info/version"):
                with open("/opt/rocm/.info/version") as f:
                    version = f.read().strip()
            else:
                out = subprocess.check_output(["hipcc", "--version"], encoding="utf-8")
                version = "HIPCC Detected"
        except: pass
    return version

def get_system_snapshot(platform_name):
    """Aggregates all system info into a dictionary."""
    snapshot = {
        "timestamp": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "os_info": {
            "system": platform.system(),
            "release": platform.release(), # Kernel Version
            "version": platform.version(),
            "arch": platform.machine(),
        },
        "cpu_info": "psutil_missing",
        "ram_info": "psutil_missing",
        "gpu_static_info": get_gpu_static_info(),
        "toolkit_version": get_toolkit_version(platform_name)
    }

    if psutil:
        vm = psutil.virtual_memory()
        snapshot["cpu_info"] = {
            "physical_cores": psutil.cpu_count(logical=False),
            "logical_cores": psutil.cpu_count(logical=True),
            "freq": f"{psutil.cpu_freq().current:.2f}Mhz" if psutil.cpu_freq() else "N/A"
        }
        snapshot["ram_info"] = {
            "total": get_size(vm.total),
            "available": get_size(vm.available)
        }

    return snapshot

# --- Core Logic ---

def detect_platform():
    if subprocess.run("which hipcc", shell=True, stdout=subprocess.DEVNULL).returncode == 0:
        return "HIP"
    if subprocess.run("which nvcc", shell=True, stdout=subprocess.DEVNULL).returncode == 0:
        return "CUDA"
    return "UNKNOWN"

def build_kernels(platform):
    log(f"Detected Platform: {platform}. Compiling kernels...")
    ensure_dir(BUILD_DIR)
    cmd = ["make", f"PLATFORM={platform}"]
    result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
    if result.returncode != 0:
        print("Build Failed:\n" + result.stderr)
        sys.exit(1)
    log("Build Complete.")

def run_test(test_name, gpu_ids, duration, mem_pct):
    config = TEST_REGISTRY[test_name]
    binary = os.path.join(BUILD_DIR, config["bin"])
    procs = []
    
    for gpu in gpu_ids:
        # Command: ./bin <gpu> <duration> <mem_pct> [args]
        cmd = [binary, str(gpu), str(duration), str(mem_pct)] + config["args"]
        log(f"Launching {test_name} on GPU {gpu} (Alloc: {mem_pct}% VRAM)...")
        p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        procs.append((gpu, p))

    return procs

def main():
    parser = argparse.ArgumentParser(description="PANTHEON: Universal GPU Stress Suite")
    parser.add_argument("--test", type=str, default="all", help=f"Test to run: {', '.join(TEST_REGISTRY.keys())} or 'all'")
    parser.add_argument("--duration", type=int, default=30, help="Duration in seconds per test")
    parser.add_argument("--gpu", type=str, default="all", help="Comma separated list of GPU IDs (e.g. 0,1) or 'all'")
    parser.add_argument("--mem", type=int, default=99, help="Percentage of free VRAM to use (Default: 99)")
    args = parser.parse_args()

    # --- Setup ---
    platform = detect_platform()
    if platform == "UNKNOWN": sys.exit("Error: No compiler found.")
    
    build_kernels(platform)
    monitor = HardwareMonitor(platform)
    
    # --- GPU Discovery ---
    avail_count = monitor.get_gpu_count()
    if args.gpu == "all":
        target_gpus = list(range(avail_count))
    else:
        target_gpus = [int(x) for x in args.gpu.split(",")]

    # --- Result Folder Setup ---
    timestamp_str = datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
    run_dir = os.path.join(RESULTS_BASE_DIR, timestamp_str)
    ensure_dir(run_dir)
    ensure_dir(DATABASE_DIR)
    
    log(f"Run ID: {timestamp_str}")

    # --- Test Selection ---
    if args.test == "all":
        queue = list(TEST_REGISTRY.keys())
    else:
        queue = [args.test]

    # --- Main Loop ---
    final_results = []

    for test in queue:
        log(f"--- STARTING TEST: {test.upper()} ---")
        
        # 1. Start Monitor
        monitor.start_collection(target_gpus)

        # 2. Run Kernels
        procs = run_test(test, target_gpus, args.duration, args.mem)
        
        # 3. Wait
        try:
            for gpu, p in procs: p.wait()
        except KeyboardInterrupt:
            for gpu, p in procs: p.kill()
            break
        
        # 4. Stop Monitor & Get Stats
        hw_stats = monitor.stop_collection() 

        # 5. Parse Kernel Output
        for gpu, p in procs:
            out, err = p.communicate()
            throughput = "N/A"
            
            if out:
                # print(out) # DEBUG: Uncomment if you still see N/A
                for line in out.split('\n'):
                    if "Throughput:" in line:
                        try:
                            # Split by ":", take the right side
                            raw_val = line.split(":")[1].strip()
                            # Extract just the number (remove GB/s, MAPS, etc)
                            number_part = raw_val.split(' ')[0] 
                            throughput = float(number_part)
                        except: 
                            pass
            
            stats = hw_stats.get(gpu, {})
            row = {
                "Test Name": test,
                "Description": TEST_REGISTRY[test]["desc"],
                "GPU ID": gpu,
                "Duration (s)": args.duration,
                "Mem Usage (%)": args.mem,
                "Throughput (GB/s)": throughput,
                "Avg Temp (C)": stats.get("avg_temp", 0),
                "Max Temp (C)": stats.get("max_temp", 0),
                "Avg Power (W)": stats.get("avg_pwr", 0),
                "Max Power (W)": stats.get("max_pwr", 0),
                "Avg Clock (MHz)": stats.get("avg_clk", 0)
            }
            final_results.append(row)
            print(f"[RESULT] GPU {gpu} | {throughput} GB/s | {row['Max Temp (C)']}C Max | {row['Max Power (W)']}W Max")

        log(f"--- FINISHED TEST: {test.upper()} ---\n")

    # --- Report Generation ---
    df = pd.DataFrame(final_results)
    print("\n" + "="*80)
    print("FINAL SUMMARY REPORT")
    print("="*80)
    print(df.drop(columns=["Description"]).to_string(index=False))
    print("="*80)

    # 1. Save Simple Reports (CSV/Excel) in results/ folder
    csv_path = os.path.join(run_dir, "summary.csv")
    xlsx_path = os.path.join(run_dir, "summary.xlsx")
    df.to_csv(csv_path, index=False)
    try: df.to_excel(xlsx_path, index=False)
    except: pass

    # 2. Save Full Database Snapshot (JSON) in database/ folder
    log("Generating Database Snapshot...")
    full_snapshot = get_system_snapshot(platform)
    full_snapshot["test_results"] = final_results
    
    # Filename: pantheon_report_<timestamp>.json
    db_file = os.path.join(DATABASE_DIR, f"pantheon_report_{timestamp_str}.json")
    
    with open(db_file, "w") as f:
        json.dump(full_snapshot, f, indent=4)
    
    log(f"Snapshot saved to: {db_file}")

if __name__ == "__main__":
    main()


==================================================
FILE: ./requirements.txt
==================================================
psutil
pandas
openpyxl
numpy


==================================================
FILE: ./kernels/common/mock_gpu.h
==================================================
#ifndef MOCK_GPU_H
#define MOCK_GPU_H

#include <iostream>
#include <cmath>
#include <cstring>
#include <cstdlib>

// --- MOCK TYPES ---
typedef int hipError_t;
#define hipSuccess 0
struct hipDeviceProp_t { int multiProcessorCount; };

// --- MOCK VECTOR TYPES ---
struct float4_ { float x, y, z, w; };
struct uint4_ { unsigned int x, y, z, w; };
struct float2_ { float x, y; };
typedef float4_ float4;
typedef uint4_ uint4;
typedef float2_ float2;

// --- MOCK QUALIFIERS ---
// Define these as empty strings so g++ ignores them
#define __global__
#define __device__
#define __host__
#define __forceinline__ inline
#define __launch_bounds__(x)

// --- MOCK THREADING ---
// We simulate 1 thread per block for logic testing
struct uint3 { unsigned int x, y, z; };
static uint3 threadIdx = {0,0,0};
static uint3 blockIdx = {0,0,0};
static uint3 blockDim = {1,1,1};
static uint3 gridDim = {1,1,1};

inline void __syncthreads() {} // No-op for single thread

// --- MOCK API ---
inline hipError_t hipSetDevice(int dev) { return hipSuccess; }
inline hipError_t hipGetDeviceProperties(hipDeviceProp_t* p, int d) { p->multiProcessorCount = 1; return hipSuccess; }
inline hipError_t hipDeviceSynchronize() { return hipSuccess; }
inline hipError_t hipMemGetInfo(size_t* f, size_t* t) { *f=1e9; *t=2e9; return hipSuccess; }
inline hipError_t hipMalloc(void** p, size_t s) { *p = malloc(s); return hipSuccess; }
inline hipError_t hipFree(void* p) { free(p); return hipSuccess; }
inline hipError_t hipMemset(void* p, int v, size_t s) { memset(p, v, s); return hipSuccess; }

// --- MOCK INTRINSICS ---
// Half-precision shim (treat as float)
typedef float __half2;
inline float __float2half2_rn(float f) { return f; }
inline float __hfma2(float a, float b, float c) { return a*b+c; }
inline float __hneg2(float a) { return -a; }
inline float2 __half22float2(float a) { return {a, a}; }

// Constructors
inline uint4 make_uint4(unsigned int x, unsigned int y, unsigned int z, unsigned int w) { return {x,y,z,w}; }

// Atomic Shim (Single threaded = standard add)
inline void atomicAdd(unsigned int* address, int val) { *address += val; }

#endif


==================================================
FILE: ./kernels/common/hip_shim.h
==================================================
#ifndef HIP_SHIM_H
#define HIP_SHIM_H

// If compiling with NVCC (NVIDIA), map HIP -> CUDA
#ifdef __CUDACC__
    #include <cuda_runtime.h>
    #include <device_launch_parameters.h>
    #include <iostream>

    // 1. Rename Types
    #define hipError_t cudaError_t
    #define hipSuccess cudaSuccess
    #define hipDeviceProp_t cudaDeviceProp
    #define hipGetErrorString cudaGetErrorString
    #define hipStream_t cudaStream_t

    // 2. Rename Functions
    #define hipSetDevice cudaSetDevice
    #define hipGetDeviceProperties cudaGetDeviceProperties
    #define hipMalloc cudaMalloc
    #define hipFree cudaFree
    #define hipMemcpy cudaMemcpy
    #define hipMemcpyDeviceToHost cudaMemcpyDeviceToHost
    #define hipMemGetInfo cudaMemGetInfo
    #define hipDeviceSynchronize cudaDeviceSynchronize
    #define hipStreamCreate cudaStreamCreate
    #define hipStreamDestroy cudaStreamDestroy
    #define hipStreamSynchronize cudaStreamSynchronize

    // 3. Handle Kernel Launch Syntax if needed (usually compatible)
    
#else
    // If compiling with HIPCC (AMD), just use standard HIP
    #include <hip/hip_runtime.h>
#endif

#endif // HIP_SHIM_H


==================================================
FILE: ./kernels/common/common.h
==================================================
#ifndef PANTHEON_COMMON_H
#define PANTHEON_COMMON_H

#include <iostream>
#include <cstdlib>

// ==========================================
// PATH 1: CPU MOCK SIMULATION (CI/CD)
// ==========================================
#ifdef PANTHEON_MOCK
    #include "mock_gpu.h"
    
    // In Mock mode, "Launch" just calls the function directly on the CPU.
    // We ignore grid/block dimensions since we are single-threaded here.
    #define LAUNCH_KERNEL(kernel_name, grid, block, ...) kernel_name(__VA_ARGS__)

// ==========================================
// PATH 2: REAL GPU (CUDA / ROCm)
// ==========================================
#else
    // --- CROSS-PLATFORM SHIM (HIP -> CUDA) ---
    #ifdef __CUDACC__
        // NVIDIA / CUDA MODE
        #include <cuda_runtime.h>
        #include <device_launch_parameters.h>

        #define hipError_t cudaError_t
        #define hipSuccess cudaSuccess
        #define hipDeviceProp_t cudaDeviceProp
        #define hipGetErrorString cudaGetErrorString
        #define hipStream_t cudaStream_t
        #define hipSetDevice cudaSetDevice
        #define hipGetDeviceProperties cudaGetDeviceProperties
        #define hipMalloc cudaMalloc
        #define hipFree cudaFree
        #define hipMemcpy cudaMemcpy
        #define hipMemcpyDeviceToHost cudaMemcpyDeviceToHost
        #define hipMemGetInfo cudaMemGetInfo
        #define hipDeviceSynchronize cudaDeviceSynchronize
        #define hipStreamCreate cudaStreamCreate
        #define hipStreamDestroy cudaStreamDestroy
        #define hipStreamSynchronize cudaStreamSynchronize
        #define hipMemset cudaMemset 

    #else
        // AMD / ROCm MODE
        #include <hip/hip_runtime.h>
    #endif

    // Real Kernel Launch Syntax
    #define LAUNCH_KERNEL(kernel_name, grid, block, ...) kernel_name<<<grid, block>>>(__VA_ARGS__)

    // 128-bit Vector Types (GCC/Clang Vector Extensions for GPU)
    typedef float float4_ __attribute__((vector_size(16)));
    typedef unsigned int uint4_ __attribute__((vector_size(16)));
#endif


// ==========================================
// SHARED UTILITIES
// ==========================================

#define BLOCK_SIZE 256

#define CHECK(cmd) \
{ \
    hipError_t error = cmd; \
    if (error != hipSuccess) { \
        std::cerr << "[PANTHEON ERROR] Code: " << error \
                  << " (" << hipGetErrorString(error) << ")" \
                  << " at " << __FILE__ << ":" << __LINE__ << std::endl; \
        exit(1); \
    } \
}

// --- NON-TEMPORAL STORE (Bypass Cache -> Write HBM) ---
// 1. CPU Mock: Standard Store
// 2. AMD: Builtin or Cast
// 3. CUDA: PTX ASM
__device__ __host__ __forceinline__ void store_nt(void* addr, uint4 val) {
#ifdef PANTHEON_MOCK
    // CPU Mock: Just write to memory
    *(uint4*)addr = val;
#elif defined(__HIP_PLATFORM_AMD__)
    // AMD: Use builtin for NT store
    typedef unsigned int __attribute__((vector_size(16))) vec_uint4;
    __builtin_nontemporal_store(*(vec_uint4*)&val, (vec_uint4*)addr);
#elif defined(__CUDACC__)
    // NVIDIA: PTX ASM
    asm volatile("st.global.cs.v4.u32 [%0], {%1, %2, %3, %4};" 
                 :: "l"(addr), "r"(val.x), "r"(val.y), "r"(val.z), "r"(val.w));
#else
    *(uint4*)addr = val;
#endif
}

// --- NON-TEMPORAL LOAD (CRASH-PROOF VERSION) ---
// 1. CPU Mock: Standard Load
// 2. AMD: Decomposed Loads (RDNA Fix)
// 3. CUDA: PTX ASM
__device__ __host__ __forceinline__ uint4 load_nt(void* addr) {
    uint4 ret;
#ifdef PANTHEON_MOCK
    // CPU Mock: Just read memory
    ret = *(uint4*)addr;
#elif defined(__HIP_PLATFORM_AMD__)
    // RDNA CRASH FIX: Decompose into 4x 32-bit loads.
    // Attempting a single 128-bit vector load (*(uint4*)) segfaults on RDNA
    // if the buffer isn't perfectly 128-bit aligned.
    unsigned int* p = (unsigned int*)addr;
    ret.x = p[0];
    ret.y = p[1];
    ret.z = p[2];
    ret.w = p[3];
#elif defined(__CUDACC__)
    // NVIDIA: PTX Streaming Load
    asm volatile("ld.global.cs.v4.u32 {%0, %1, %2, %3}, [%4];" 
                 : "=r"(ret.x), "=r"(ret.y), "=r"(ret.z), "=r"(ret.w) : "l"(addr));
#else
    ret = *(uint4*)addr;
#endif
    return ret;
}

#endif // PANTHEON_COMMON_H


==================================================
FILE: ./kernels/atomic_virus/atomic_virus.cpp
==================================================
#include "../common/common.h"
#include <vector>
#include <chrono>

// --- ATOMIC VIRUS ---
// Hammers memory with Atomic operations using a Wide Stride.
// This forces "Read-Modify-Write" cycles that overwhelm the L2 Cache Arbiters.
__global__ void atomic_virus_kernel(uint4* data, size_t n) {
    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    size_t stride = blockDim.x * gridDim.x;

    // Cast to uint for atomic operations
    unsigned int* atom_ptr = (unsigned int*)data;
    size_t num_ints = n * 4; 

    // Stride loop
    for (size_t i = tid; i < num_ints; i += stride) {
        // Atomic Add: Forces hardware to lock the cache line, update, and unlock.
        // Extremely taxing on the internal fabric.
        atomicAdd(&atom_ptr[i], 1);
    }
}

int main(int argc, char* argv[]) {
    if (argc < 4) return 1;
    int gpu_id = atoi(argv[1]);
    int duration = atoi(argv[2]);
    int mem_pct = atoi(argv[3]);

    CHECK(hipSetDevice(gpu_id));

    size_t free, total; CHECK(hipMemGetInfo(&free, &total));
    if (mem_pct > 99) mem_pct = 99;
    
    // Use 60% VRAM. If too large, latency hides the stress. 
    // If too small, it stays in L1. 60% ensures L2 thrashing.
    size_t alloc_size = (free * mem_pct) / 100;
    size_t num_elements = alloc_size / 16; 

    uint4* d_data; CHECK(hipMalloc(&d_data, alloc_size));
    CHECK(hipMemset(d_data, 0, alloc_size)); 

    hipDeviceProp_t prop; CHECK(hipGetDeviceProperties(&prop, gpu_id));
    int num_blocks = prop.multiProcessorCount * 16;

    std::cout << "[PANTHEON] GPU " << gpu_id << ": Running ATOMIC VIRUS (L2/ROP Stress)..." << std::endl;

    auto start_time = std::chrono::high_resolution_clock::now();
    size_t ops_performed = 0;

    while (true) {
	LAUNCH_KERNEL(atomic_virus_kernel, num_blocks, BLOCK_SIZE, d_data, num_elements);
        CHECK(hipDeviceSynchronize());
        
        // 4 ints per uint4 element
        ops_performed += num_elements * 4;

        auto now = std::chrono::high_resolution_clock::now();
        if (std::chrono::duration_cast<std::chrono::seconds>(now - start_time).count() >= duration) break;
    }

    double seconds = std::chrono::duration<double>(std::chrono::high_resolution_clock::now() - start_time).count();
    // Metric: MAPS (Million Atomic Operations Per Second)
    std::cout << "Throughput: " << (ops_performed / 1e6) / seconds << " MAPS" << std::endl;

    CHECK(hipFree(d_data));
    return 0;
}


==================================================
FILE: ./kernels/compute_virus/compute_virus.cpp
==================================================
#include "../common/common.h"
#include <chrono>

// --- VOLTAGE VIRUS KERNEL ---
// Uses volatile math to force rapid ALU state switching (0->1->0)
// This targets the Logic Rail (VDD_GFX) to induce di/dt droop.
__global__ void voltage_droop_kernel(int iters, float* sink) {
    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    // Volatile prevents the compiler from pre-calculating the result
    volatile float a = 1.0f; 
    volatile float b = -1.0f;
    volatile float c = 0.5f;
    volatile float d = -0.5f;

    for(int i=0; i<iters; ++i) {
        // FMA (Fused Multiply Add) Chain
        // High power consumption density on both CDNA and Volta+ arch
        #ifdef __HIP_PLATFORM_AMD__
            a = __builtin_fma(a, b, c);
            b = __builtin_fma(b, c, d);
            c = __builtin_fma(c, d, a);
            d = __builtin_fma(d, a, b);
        #else
            // Standard C++ FMA (CUDA maps this to hardware FMA)
            a = fmaf(a, b, c);
            b = fmaf(b, c, d);
            c = fmaf(c, d, a);
            d = fmaf(d, a, b);
        #endif
        
        // Polarity Shock: Flip signs every 16 ops to maximize bit toggles
        if ((i & 0xF) == 0) {
            a = -a;
            b = -b;
        }
    }

    // Write to sink once at the end to keep the dependency chain alive
    if (a + b + c + d == 12345.0f) {
        sink[tid] = a;
    }
}

int main(int argc, char* argv[]) {
    // Expected args: ./bin <gpu> <duration> <mem_pct>
    if (argc < 4) return 1;
    
    int gpu_id = atoi(argv[1]);
    int duration = atoi(argv[2]);
    // int mem_pct = atoi(argv[3]); // Ignored: This test is Compute Bound

    CHECK(hipSetDevice(gpu_id));
    
    // Small allocation just for the sink (results)
    float* d_sink;
    CHECK(hipMalloc(&d_sink, 1024 * sizeof(float)));

    // Maximum Occupancy Strategy
    // We want every Compute Unit (CU) to be hammering the ALUs
    hipDeviceProp_t prop; CHECK(hipGetDeviceProperties(&prop, gpu_id));
    int num_blocks = prop.multiProcessorCount * 16; 

    std::cout << "[PANTHEON] GPU " << gpu_id << ": Running VOLTAGE DROOP VIRUS (ALU Hammer)..." << std::endl;

    auto start_time = std::chrono::high_resolution_clock::now();
    
    // Main Stress Loop
    while(true) {
        // Run a heavy batch of instructions
        LAUNCH_KERNEL(voltage_droop_kernel, num_blocks, BLOCK_SIZE, 20000, d_sink);
        CHECK(hipDeviceSynchronize());
        
        auto now = std::chrono::high_resolution_clock::now();
        if (std::chrono::duration_cast<std::chrono::seconds>(now - start_time).count() >= duration) break;
    }
    
    CHECK(hipFree(d_sink));
    return 0;
}


==================================================
FILE: ./kernels/compute_virus/compute_virus_agg.cpp
==================================================
#include "../common/common.h"
#include <chrono>

// --- INCINERATOR KERNEL ---
// Combines FMA (Vector Unit) + LDS Thrashing (Local Data Share).
// This creates a "Power Virus" that attacks multiple sub-units simultaneously.

__global__ void incinerator_kernel(int iters, float* sink) {
    size_t tid = threadIdx.x;
    
    // 1. Setup Shared Memory (LDS)
    // We declare a volatile array to force high-speed SRAM access
    __shared__ volatile float smem[BLOCK_SIZE];
    
    // Init LDS
    smem[tid] = (float)tid;
    __syncthreads();

    // Registers for FMA
    volatile float a = 1.0f; 
    volatile float b = -1.0f;
    volatile float c = 0.5f;

    for(int i=0; i<iters; ++i) {
        // --- VECTOR UNIT STRESS (FMA) ---
        #ifdef __HIP_PLATFORM_AMD__
            a = __builtin_fma(a, b, c);
            b = __builtin_fma(b, c, a);
            c = __builtin_fma(c, a, b);
        #else
            a = fmaf(a, b, c);
            b = fmaf(b, c, a);
            c = fmaf(c, a, b);
        #endif
        
        // --- SRAM STRESS (LDS) ---
        // We read from shared memory, modify it, and write back.
        // We use an XOR index to cause bank conflicts (on purpose) for heat.
        int idx = tid ^ 1; // Swap neighbors
        float val = smem[idx];
        smem[tid] = val + 1.0f; // Write back
        
        // --- POLARITY SHOCK ---
        if ((i & 0xF) == 0) {
            a = -a;
        }
    }

    if (a + b + c + smem[tid] == 12345.0f) {
        sink[0] = a;
    }
}

int main(int argc, char* argv[]) {
    if (argc < 4) return 1;
    int gpu_id = atoi(argv[1]);
    int duration = atoi(argv[2]);

    CHECK(hipSetDevice(gpu_id));
    float* d_sink; CHECK(hipMalloc(&d_sink, 1024 * sizeof(float)));

    // Max Occupancy
    hipDeviceProp_t prop; CHECK(hipGetDeviceProperties(&prop, gpu_id));
    int num_blocks = prop.multiProcessorCount * 16; 

    std::cout << "[PANTHEON] GPU " << gpu_id << ": Running INCINERATOR (FMA + LDS Stress)..." << std::endl;

    auto start_time = std::chrono::high_resolution_clock::now();
    
    while(true) {
        LAUNCH_KERNEL(incinerator_kernel, num_blocks, BLOCK_SIZE, 20000, d_sink);
	CHECK(hipDeviceSynchronize());
        
        auto now = std::chrono::high_resolution_clock::now();
        if (std::chrono::duration_cast<std::chrono::seconds>(now - start_time).count() >= duration) break;
    }
    
    CHECK(hipFree(d_sink));
    return 0;
}


==================================================
FILE: ./kernels/cache_latency/cache_latency.cpp
==================================================
#include "../common/common.h"
#include <vector>
#include <chrono>

// Linear Congruential Generator constants for full-period traversal
// This ensures we visit every slot exactly once in random order.
#define LCG_A 1664525
#define LCG_C 1013904223

// --- INITIALIZATION KERNEL ---
// Pre-calculates the "Next Hop" index for every memory slot
__global__ void init_lcg_kernel(size_t* data, size_t n) {
    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    size_t stride = blockDim.x * gridDim.x;
    
    // data[i] = (A*i + C) % n
    // Since n is a power of 2, we use bitwise AND for modulo
    for (size_t i = tid; i < n; i += stride) {
        data[i] = (LCG_A * i + LCG_C) & (n - 1); 
    }
}

// --- STRESS KERNEL (POINTER CHASER) ---
// Forces the GPU to wait for memory latency on every instruction.
// Cannot be prefetched by hardware.
__global__ void latency_kernel(size_t* data, size_t n, int loops, size_t* sink) {
    size_t tid = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (tid < n) {
        size_t idx = tid;
        
        // Critical dependency path: idx = data[idx]
        for (int i = 0; i < loops; ++i) {
            idx = data[idx];
        }
        
        sink[tid] = idx;
    }
}

int main(int argc, char* argv[]) {
    // Expected args: ./bin <gpu> <duration> <mem_pct>
    if (argc < 4) return 1;
    
    int gpu_id = atoi(argv[1]);
    int duration = atoi(argv[2]);
    int mem_pct = atoi(argv[3]);

    CHECK(hipSetDevice(gpu_id));

    // Dynamic Allocation based on %
    // For latency tests, we just need > L2 Cache Size (usually > 128MB).
    // Using 99% forces the random walk to cross ALL memory channels/banks/stacks.
    size_t free, total;
    CHECK(hipMemGetInfo(&free, &total));
    if (mem_pct > 99) mem_pct = 99;
    if (mem_pct < 1) mem_pct = 1;
    
    // Align to power of 2 for fast LCG modulo
    size_t raw_size = (free * mem_pct) / 100;
    size_t num_elements = 1;
    while (num_elements * 2 * sizeof(size_t) <= raw_size) {
        num_elements *= 2;
    }
    
    size_t alloc_size = num_elements * sizeof(size_t);

    size_t* d_data; CHECK(hipMalloc(&d_data, alloc_size));
    size_t* d_sink; CHECK(hipMalloc(&d_sink, alloc_size));

    // Grid config
    hipDeviceProp_t prop; CHECK(hipGetDeviceProperties(&prop, gpu_id));
    int num_blocks = prop.multiProcessorCount * 4; 

    // 1. Initialize Random Table
    std::cout << "[PANTHEON] GPU " << gpu_id << ": Init Random Walk on " 
              << alloc_size / (1024*1024) << " MB (" << num_elements << " nodes)..." << std::endl;
    
    LAUNCH_KERNEL(init_lcg_kernel, num_blocks, BLOCK_SIZE, d_data, num_elements);
    CHECK(hipDeviceSynchronize());    

    // 2. Run Stress
    std::cout << "[PANTHEON] GPU " << gpu_id << ": Running CACHE LATENCY STRESS..." << std::endl;
    auto start_time = std::chrono::high_resolution_clock::now();
    
    // Inner loops: High count to minimize kernel launch overhead
    int inner_loops = 10000; 

    while (true) {
        LAUNCH_KERNEL(latency_kernel, num_blocks, BLOCK_SIZE, d_data, num_elements, inner_loops, d_sink);
	CHECK(hipDeviceSynchronize());
        
        auto now = std::chrono::high_resolution_clock::now();
        if (std::chrono::duration_cast<std::chrono::seconds>(now - start_time).count() >= duration) break;
    }

    CHECK(hipFree(d_data));
    CHECK(hipFree(d_sink));
    return 0;
}


==================================================
FILE: ./kernels/hbm_write/hbm_write_agg.cpp
==================================================
#include "../common/common.h"
#include <vector>
#include <chrono>

// --- AGGRESSIVE WRITE KERNEL ---
// Changes from Standard:
// 1. Crosstalk Pattern (0x55... vs 0xAA...)
// 2. 64x Unroll Factor (Max Bus Saturation)
// 3. Strict Launch Bounds

__global__ __launch_bounds__(256)
void hbm_write_agg_kernel(uint4* data, size_t n) {
    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;
    size_t stride = blockDim.x * gridDim.x;

    // Pattern A: 10101010... (0xAAAAAAAA)
    // Pattern B: 01010101... (0x55555555)
    // Switching between these flips EVERY bit on the bus relative to its neighbor.
    // This creates maximum Inter-Symbol Interference (ISI) and Crosstalk.
    uint4 pA = make_uint4(0xAAAAAAAA, 0xAAAAAAAA, 0xAAAAAAAA, 0xAAAAAAAA);
    uint4 pB = make_uint4(0x55555555, 0x55555555, 0x55555555, 0x55555555);

    // Massive 64x Unroll
    // We want the instruction pipeline to be nothing but "STORE, STORE, STORE"
    for (size_t i = idx; i + stride * 63 < n; i += stride * 64) {
        store_nt(&data[i],    pA); store_nt(&data[i+1],  pB);
        store_nt(&data[i+2],  pA); store_nt(&data[i+3],  pB);
        store_nt(&data[i+4],  pA); store_nt(&data[i+5],  pB);
        store_nt(&data[i+6],  pA); store_nt(&data[i+7],  pB);
        store_nt(&data[i+8],  pA); store_nt(&data[i+9],  pB);
        store_nt(&data[i+10], pA); store_nt(&data[i+11], pB);
        store_nt(&data[i+12], pA); store_nt(&data[i+13], pB);
        store_nt(&data[i+14], pA); store_nt(&data[i+15], pB);
        
        // Block 2
        store_nt(&data[i+16], pA); store_nt(&data[i+17], pB);
        store_nt(&data[i+18], pA); store_nt(&data[i+19], pB);
        store_nt(&data[i+20], pA); store_nt(&data[i+21], pB);
        store_nt(&data[i+22], pA); store_nt(&data[i+23], pB);
        store_nt(&data[i+24], pA); store_nt(&data[i+25], pB);
        store_nt(&data[i+26], pA); store_nt(&data[i+27], pB);
        store_nt(&data[i+28], pA); store_nt(&data[i+29], pB);
        store_nt(&data[i+30], pA); store_nt(&data[i+31], pB);

        // Block 3
        store_nt(&data[i+32], pA); store_nt(&data[i+33], pB);
        store_nt(&data[i+34], pA); store_nt(&data[i+35], pB);
        store_nt(&data[i+36], pA); store_nt(&data[i+37], pB);
        store_nt(&data[i+38], pA); store_nt(&data[i+39], pB);
        store_nt(&data[i+40], pA); store_nt(&data[i+41], pB);
        store_nt(&data[i+42], pA); store_nt(&data[i+43], pB);
        store_nt(&data[i+44], pA); store_nt(&data[i+45], pB);
        store_nt(&data[i+46], pA); store_nt(&data[i+47], pB);

        // Block 4
        store_nt(&data[i+48], pA); store_nt(&data[i+49], pB);
        store_nt(&data[i+50], pA); store_nt(&data[i+51], pB);
        store_nt(&data[i+52], pA); store_nt(&data[i+53], pB);
        store_nt(&data[i+54], pA); store_nt(&data[i+55], pB);
        store_nt(&data[i+56], pA); store_nt(&data[i+57], pB);
        store_nt(&data[i+58], pA); store_nt(&data[i+59], pB);
        store_nt(&data[i+60], pA); store_nt(&data[i+61], pB);
        store_nt(&data[i+62], pA); store_nt(&data[i+63], pB);

        // Standard pointer arithmetic handled by loop
    }
}

int main(int argc, char* argv[]) {
    if (argc < 4) return 1;
    int gpu_id = atoi(argv[1]);
    int duration = atoi(argv[2]);
    int mem_pct = atoi(argv[3]);

    CHECK(hipSetDevice(gpu_id));

    size_t free, total;
    CHECK(hipMemGetInfo(&free, &total));
    if (mem_pct > 99) mem_pct = 99;
    size_t alloc_size = (free * mem_pct) / 100;
    size_t num_elements = alloc_size / 16; 

    void* d_data;
    CHECK(hipMalloc(&d_data, alloc_size));

    // Oversubscribe CUs
    hipDeviceProp_t prop; CHECK(hipGetDeviceProperties(&prop, gpu_id));
    int num_blocks = prop.multiProcessorCount * 20;

    std::cout << "[PANTHEON] GPU " << gpu_id << ": HBM AGGRESSIVE WRITE (Crosstalk) | " 
              << mem_pct << "% VRAM | " << duration << "s" << std::endl;

    auto start_time = std::chrono::high_resolution_clock::now();
    size_t bytes_transferred = 0;

    while (true) {
	LAUNCH_KERNEL(hbm_write_agg_kernel, num_blocks, BLOCK_SIZE, d_data, num_elements);
        CHECK(hipDeviceSynchronize());
        
        bytes_transferred += alloc_size;

        auto now = std::chrono::high_resolution_clock::now();
        if (std::chrono::duration_cast<std::chrono::seconds>(now - start_time).count() >= duration) break;
    }

    double seconds = std::chrono::duration<double>(std::chrono::high_resolution_clock::now() - start_time).count();
    std::cout << "Throughput: " << (bytes_transferred / 1e9) / seconds << " GB/s" << std::endl;

    CHECK(hipFree(d_data));
    return 0;
}


==================================================
FILE: ./kernels/hbm_write/hbm_write.cpp
==================================================
#include "../common/common.h"
#include <vector>
#include <chrono>

// --- WRITE KERNEL ---
// Uses Non-Temporal Stores to fill HBM bandwidth without polluting L2
__global__ void hbm_write_kernel(uint4* data, size_t n) {
    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;
    size_t stride = blockDim.x * gridDim.x;

    // Rail-to-Rail Pattern (0x00 <-> 0xFF)
    uint4 p0 = make_uint4(0x00000000, 0x00000000, 0x00000000, 0x00000000);
    uint4 p1 = make_uint4(0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF, 0xFFFFFFFF);

    // Unroll 16x
    for (size_t i = idx; i + stride * 15 < n; i += stride * 16) {
        store_nt(&data[i], p0);
        store_nt(&data[i+stride], p1);
        store_nt(&data[i+stride*2], p0);
        store_nt(&data[i+stride*3], p1);
        store_nt(&data[i+stride*4], p0);
        store_nt(&data[i+stride*5], p1);
        store_nt(&data[i+stride*6], p0);
        store_nt(&data[i+stride*7], p1);
        store_nt(&data[i+stride*8], p0);
        store_nt(&data[i+stride*9], p1);
        store_nt(&data[i+stride*10], p0);
        store_nt(&data[i+stride*11], p1);
        store_nt(&data[i+stride*12], p0);
        store_nt(&data[i+stride*13], p1);
        store_nt(&data[i+stride*14], p0);
        store_nt(&data[i+stride*15], p1);
    }
}

int main(int argc, char* argv[]) {
    if (argc < 4) return 1;
    int gpu_id = atoi(argv[1]);
    int duration = atoi(argv[2]);
    int mem_pct = atoi(argv[3]);

    CHECK(hipSetDevice(gpu_id));

    size_t free, total;
    CHECK(hipMemGetInfo(&free, &total));
    if (mem_pct > 99) mem_pct = 99;
    size_t alloc_size = (free * mem_pct) / 100;
    size_t num_elements = alloc_size / 16; 

    void* d_data;
    CHECK(hipMalloc(&d_data, alloc_size));

    // Oversubscribe CUs
    hipDeviceProp_t prop; CHECK(hipGetDeviceProperties(&prop, gpu_id));
    int num_blocks = prop.multiProcessorCount * 20;

    std::cout << "[PANTHEON] GPU " << gpu_id << ": HBM WRITE (NT Store) | " 
              << mem_pct << "% VRAM | " << duration << "s" << std::endl;

    auto start_time = std::chrono::high_resolution_clock::now();
    size_t bytes_transferred = 0;

    while (true) {
	LAUNCH_KERNEL(hbm_write_kernel, num_blocks, BLOCK_SIZE, d_data, num_elements);
        CHECK(hipDeviceSynchronize());
        
        bytes_transferred += alloc_size; // Write only

        auto now = std::chrono::high_resolution_clock::now();
        if (std::chrono::duration_cast<std::chrono::seconds>(now - start_time).count() >= duration) break;
    }

    double seconds = std::chrono::duration<double>(std::chrono::high_resolution_clock::now() - start_time).count();
    std::cout << "Throughput: " << (bytes_transferred / 1e9) / seconds << " GB/s" << std::endl;

    CHECK(hipFree(d_data));
    return 0;
}


==================================================
FILE: ./kernels/hbm_read/hbm_read_agg.cpp
==================================================
#include "../common/common.h"
#include <vector>
#include <chrono>

// --- SAFE READ KERNEL ---
// Uses volatile pointer access to prevent compiler optimization
// without using unstable intrinsics.
__global__ __launch_bounds__(256)
void hbm_read_agg_kernel(uint4* data, size_t n, uint4* sink) {
    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;
    size_t stride = blockDim.x * gridDim.x;

    unsigned int a0 = 0, a1 = 0, a2 = 0, a3 = 0;
    
    // Cast to volatile uint for safe, unoptimized reads
    volatile unsigned int* ptr = (volatile unsigned int*)data;
    size_t num_ints = n * 4; // uint4 = 4 uints
    size_t step = stride * 4; // Stride in uints

    // Manual 16x Unroll
    for (size_t i = idx * 4; i + step * 15 < num_ints; i += step * 16) {
        a0 += ptr[i];
        a1 += ptr[i + step];
        a2 += ptr[i + step * 2];
        a3 += ptr[i + step * 3];
        
        a0 += ptr[i + step * 4];
        a1 += ptr[i + step * 5];
        a2 += ptr[i + step * 6];
        a3 += ptr[i + step * 7];

        a0 += ptr[i + step * 8];
        a1 += ptr[i + step * 9];
        a2 += ptr[i + step * 10];
        a3 += ptr[i + step * 11];

        a0 += ptr[i + step * 12];
        a1 += ptr[i + step * 13];
        a2 += ptr[i + step * 14];
        a3 += ptr[i + step * 15];
    }

    // Write result to sink (One slot per thread)
    // idx is guaranteed to be < GridDim * BlockDim, so we map 1:1 to sink
    unsigned int final_sum = a0 + a1 + a2 + a3;
    if (final_sum == 0xDEADBEEF) sink[idx] = make_uint4(final_sum, 0, 0, 0);
}

// Helper to init data
__global__ void init_data(uint4* data, size_t n) {
    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;
    size_t stride = blockDim.x * gridDim.x;
    uint4 val = make_uint4(1, 1, 1, 1);
    for (size_t i = idx; i < n; i += stride) data[i] = val;
}

int main(int argc, char* argv[]) {
    if (argc < 4) return 1;
    int gpu_id = atoi(argv[1]);
    int duration = atoi(argv[2]);
    int mem_pct = atoi(argv[3]);

    CHECK(hipSetDevice(gpu_id));

    // 1. Calculate Available Memory
    size_t free, total; CHECK(hipMemGetInfo(&free, &total));
    if (mem_pct > 99) mem_pct = 99;
    
    // 2. Alloc Source Data (The massive buffer)
    // We leave 50MB padding to be safe against driver overhead
    size_t alloc_size = (free * mem_pct) / 100;
    if (free - alloc_size < 50 * 1024 * 1024) {
        alloc_size = free - 50 * 1024 * 1024;
    }
    size_t num_elements = alloc_size / 16; 

    // 3. Alloc Sink (Small buffer, just for results)
    hipDeviceProp_t prop; CHECK(hipGetDeviceProperties(&prop, gpu_id));
    int num_blocks = prop.multiProcessorCount * 20;
    size_t sink_size = num_blocks * BLOCK_SIZE * sizeof(uint4);

    uint4* d_data; CHECK(hipMalloc(&d_data, alloc_size));
    uint4* d_sink; CHECK(hipMalloc(&d_sink, sink_size)); // <-- FIX: Use small size

    // Init Data
    LAUNCH_KERNEL(init_data, num_blocks, BLOCK_SIZE, d_data, num_elements);
    CHECK(hipDeviceSynchronize());

    std::cout << "[PANTHEON] GPU " << gpu_id << ": HBM READ AGG | " 
              << mem_pct << "% VRAM | " << duration << "s" << std::endl;

    auto start_time = std::chrono::high_resolution_clock::now();
    size_t bytes_transferred = 0;

    while (true) {
	LAUNCH_KERNEL(hbm_read_agg_kernel, num_blocks, BLOCK_SIZE, d_data, num_elements);
        CHECK(hipDeviceSynchronize());
        
        bytes_transferred += alloc_size;

        auto now = std::chrono::high_resolution_clock::now();
        if (std::chrono::duration_cast<std::chrono::seconds>(now - start_time).count() >= duration) break;
    }

    double seconds = std::chrono::duration<double>(std::chrono::high_resolution_clock::now() - start_time).count();
    std::cout << "Throughput: " << (bytes_transferred / 1e9) / seconds << " GB/s" << std::endl;

    CHECK(hipFree(d_data)); CHECK(hipFree(d_sink));
    return 0;
}


==================================================
FILE: ./kernels/hbm_read/hbm_read.cpp
==================================================
#include "../common/common.h"
#include <vector>
#include <chrono>

// --- STANDARD READ KERNEL (RDNA FIX) ---
// Uses volatile reads decomposed into 32-bit chunks to prevent crashes on AMD.
__global__ void hbm_read_kernel(uint4* data, size_t n, uint4* sink) {
    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;
    size_t stride = blockDim.x * gridDim.x;

    unsigned int acc = 0;
    
    // Cast to volatile uint
    volatile unsigned int* ptr = (volatile unsigned int*)data;
    size_t num_ints = n * 4; 
    size_t step = stride * 4;

    // 4x Unroll (Standard)
    for (size_t i = idx * 4; i + step * 3 < num_ints; i += step * 4) {
        acc += ptr[i];
        acc += ptr[i + step];
        acc += ptr[i + step * 2];
        acc += ptr[i + step * 3];
    }

    if (acc == 0xDEADBEEF) sink[idx] = make_uint4(acc, 0, 0, 0);
}

// Helper to init data
__global__ void init_data(uint4* data, size_t n) {
    size_t idx = blockIdx.x * blockDim.x + threadIdx.x;
    size_t stride = blockDim.x * gridDim.x;
    uint4 val = make_uint4(1, 1, 1, 1);
    for (size_t i = idx; i < n; i += stride) data[i] = val;
}

int main(int argc, char* argv[]) {
    if (argc < 4) return 1;
    int gpu_id = atoi(argv[1]);
    int duration = atoi(argv[2]);
    int mem_pct = atoi(argv[3]);

    CHECK(hipSetDevice(gpu_id));

    size_t free, total; CHECK(hipMemGetInfo(&free, &total));
    if (mem_pct > 99) mem_pct = 99;
    
    // Safe Allocation Logic
    size_t alloc_size = (free * mem_pct) / 100;
    if (free - alloc_size < 50 * 1024 * 1024) alloc_size = free - 50 * 1024 * 1024;
    size_t num_elements = alloc_size / 16; 

    // Small Sink Allocation
    hipDeviceProp_t prop; CHECK(hipGetDeviceProperties(&prop, gpu_id));
    int num_blocks = prop.multiProcessorCount * 20;
    size_t sink_size = num_blocks * BLOCK_SIZE * sizeof(uint4);

    uint4* d_data; CHECK(hipMalloc(&d_data, alloc_size));
    uint4* d_sink; CHECK(hipMalloc(&d_sink, sink_size));

    LAUNCH_KERNEL(init_data, num_blocks, BLOCK_SIZE, d_data, num_elements);
    CHECK(hipDeviceSynchronize());

    std::cout << "[PANTHEON] GPU " << gpu_id << ": HBM READ (Standard) | " 
              << mem_pct << "% VRAM | " << duration << "s" << std::endl;

    auto start_time = std::chrono::high_resolution_clock::now();
    size_t bytes_transferred = 0;

    while (true) {
	LAUNCH_KERNEL(hbm_read_kernel, num_blocks, BLOCK_SIZE, d_data, num_elements);
        CHECK(hipDeviceSynchronize());
        bytes_transferred += alloc_size;
        auto now = std::chrono::high_resolution_clock::now();
        if (std::chrono::duration_cast<std::chrono::seconds>(now - start_time).count() >= duration) break;
    }

    double seconds = std::chrono::duration<double>(std::chrono::high_resolution_clock::now() - start_time).count();
    std::cout << "Throughput: " << (bytes_transferred / 1e9) / seconds << " GB/s" << std::endl;

    CHECK(hipFree(d_data)); CHECK(hipFree(d_sink));
    return 0;
}


==================================================
FILE: ./kernels/tensor_virus/tensor_virus.cpp
==================================================
#include "../common/common.h"
#include <chrono>

// --- HEADER FIX FOR CUDA/HIP ---
#ifdef __CUDACC__
    #include <cuda_fp16.h>
#else
    #include <hip/hip_fp16.h>
#endif

// --- TENSOR VIRUS (FP16 HAMMER) ---
// Uses Half-Precision (FP16) to saturate Tensor/Matrix cores.
__global__ void tensor_virus_kernel(int iters, float* sink) {
    size_t tid = threadIdx.x;
    
    // Initialize Half-Precision Registers
    // __half2 is a vector of two 16-bit floats (Packed FP16)
#ifdef __CUDACC__
    __half2 a = __float2half2_rn(1.0f);
    __half2 b = __float2half2_rn(0.5f);
    __half2 c = __float2half2_rn(-1.0f);
#else
    // AMD/ROCm often defines __half2 constructors differently in older versions,
    // but newer HIP supports the standard CUDA-style naming:
    __half2 a = __float2half2_rn(1.0f);
    __half2 b = __float2half2_rn(0.5f);
    __half2 c = __float2half2_rn(-1.0f);
#endif

    for(int i=0; i<iters; ++i) {
        // Universal FMA (Fused Multiply Add)
        // This works on both AMD (ROCm) and NVIDIA (CUDA)
        // It maps to v_pk_fma_f16 on AMD and HMMA/HFMA on NVIDIA.
        a = __hfma2(a, b, c);
        b = __hfma2(b, c, a);
        c = __hfma2(c, a, b);

        // Polarity flip to prevent value convergence
        if ((i & 0xF) == 0) {
            a = __hneg2(a);
        }
    }

    // Convert back to float to write sink
    float2 res = __half22float2(a);
    if (res.x + res.y == 12345.0f) sink[tid] = res.x;
}

int main(int argc, char* argv[]) {
    if (argc < 4) return 1;
    int gpu_id = atoi(argv[1]);
    int duration = atoi(argv[2]);

    CHECK(hipSetDevice(gpu_id));
    float* d_sink; CHECK(hipMalloc(&d_sink, 1024 * sizeof(float)));

    hipDeviceProp_t prop; CHECK(hipGetDeviceProperties(&prop, gpu_id));
    
    // Max Occupancy: Tensor tests need MASSIVE parallelism to fill the huge pipes.
    // For RDNA/gfx1010, we want to saturate the vector ALUs.
    int num_blocks = prop.multiProcessorCount * 32; 

    std::cout << "[PANTHEON] GPU " << gpu_id << ": Running TENSOR VIRUS (FP16 Stress)..." << std::endl;

    auto start_time = std::chrono::high_resolution_clock::now();
    
    while(true) {
	LAUNCH_KERNEL(tensor_virus_kernel, num_blocks, BLOCK_SIZE, 20000, d_sink);
        CHECK(hipDeviceSynchronize());
        
        auto now = std::chrono::high_resolution_clock::now();
        if (std::chrono::duration_cast<std::chrono::seconds>(now - start_time).count() >= duration) break;
    }
    
    CHECK(hipFree(d_sink));
    return 0;
}

